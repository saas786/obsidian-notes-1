ReLU is an [[activation function]] where for an input $x$, the output $\phi(x)$ equals $x$ when $x>0$, and equals $0$ when $x\leq 0$.


![[Pasted image 20211204184720.png]]